// AUTOGENERATED COPYRIGHT HEADER START
// Copyright (C) 2023-2024 Michael Fabian 'Xaymar' Dirks <info@xaymar.com>
// AUTOGENERATED COPYRIGHT HEADER END
// Copyright 2023 Michael Fabian 'Xaymar' Dirks <info@xaymar.com>

#include "ringbuffer.hpp"
#include "core.hpp"

#include "warning-disable.hpp"
#include <algorithm>
#include <stdexcept>

#ifdef _WIN32
#include <Windows.h>
// Must be after Windows.h
#include <VersionHelpers.h>
// Fix missing VirtualAlloc2
#pragma comment(lib, "mincore")
#else
#include <unistd.h>
#endif

#include "warning-enable.hpp"

size_t get_minimum_page_size()
{
#ifdef _WIN32
	SYSTEM_INFO info = {0};
	GetSystemInfo(&info);
	return static_cast<size_t>(std::max(info.dwPageSize, info.dwAllocationGranularity));
#else
	return static_cast<size_t>(getpagesize());
#endif
}

struct internal_data {
#ifdef _WIN32
	std::shared_ptr<void> area  = nullptr;
	std::shared_ptr<void> left  = nullptr;
	std::shared_ptr<void> right = nullptr;
#else
#endif
};

#ifdef _WIN32
// This only exists because std::unique_ptr needs it, while std::shared_ptr does not need it. Fuck C++'s inconsistencies...
struct virtualfree {
	void operator()(void* ptr)
	{
		VirtualFree(ptr, 0, MEM_RELEASE);
	}
};

#endif
template<typename T>
tonplugins::memory::ring<T>::ring(size_t size) : _write_pos(0), _read_pos(0), _notifications(), _notification_id(0)
{
	// Calculate the proper size.
	size_t page = get_minimum_page_size();
	_size       = size;
	_size *= sizeof(T); // Convert to Bytes
	_size += (page - 1); // Prepare for rounding up
	_size /= page; // Round towards zero and convert to Pages
	_size *= page; // Convert to Bytes
	_size /= sizeof(T); // Convert to Elements.
	size_t real_size = _size * sizeof(T);
	size_t wide_size = real_size * 2;

	// Allocate the internal data structure.
	auto id        = std::make_shared<internal_data>();
	_internal_data = id;

#ifdef _WIN32
	constexpr uint64_t max_attempts = 255;

	if (IsWindows10OrGreater()) {
		// This can unfortunately fail, so we should retry a lot.
		bool done = false;
		for (uint64_t attempt = 1; !done && (attempt <= max_attempts); attempt++) {
			// Create a file mapping backed by the paging file.
			void* filemap = CreateFileMappingW(INVALID_HANDLE_VALUE, nullptr, PAGE_READWRITE, ((real_size >> 32) & 0xFFFFFFFFull), (real_size & 0xFFFFFFFFull), nullptr);
			if (!filemap) {
				CLOG_THIS("Attempt %llu/%llu: CreateFileMappingW failed with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}
			id->area = std::shared_ptr<void>{filemap, [](void* ptr) { CloseHandle(ptr); }};

			// Reserve the continuous memory region.
			std::unique_ptr<void, virtualfree> placeholder = nullptr;
			void*                              area        = VirtualAlloc2(nullptr, nullptr, static_cast<SIZE_T>(wide_size), MEM_RESERVE | MEM_RESERVE_PLACEHOLDER, PAGE_READWRITE, nullptr, 0);
			if (!area) {
				CLOG_THIS("Attempt %llu/%llu: VirtualAlloc2 failed with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}
			placeholder = std::unique_ptr<void, virtualfree>(area);

			// Split the reserved area in half.
#pragma warning(push)
#pragma warning(disable : 28160)
#pragma warning(disable : 6333)
			if (VirtualFree(placeholder.get(), static_cast<SIZE_T>(real_size), MEM_RELEASE | MEM_PRESERVE_PLACEHOLDER)) {
#pragma warning(pop)
				CLOG_THIS("Attempt %llu/%llu: VirtualFree failed to split reserved memory with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}

			// Map left half.
			void* left = MapViewOfFile3(id->area.get(), nullptr, reinterpret_cast<uint8_t*>(placeholder.get()), 0, real_size, MEM_REPLACE_PLACEHOLDER, PAGE_READWRITE, nullptr, 0);
			if (!left) {
				CLOG_THIS("Attempt %llu/%llu: MapViewOfFile3 for left half failed with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}
			id->left = std::shared_ptr<void>(left, [](void* ptr) { UnmapViewOfFile(ptr); });

			// We've transferred ownership of the first half to MapViewOfFile3, so we no longer need to free it.
			placeholder = std::unique_ptr<void, virtualfree>(reinterpret_cast<uint8_t*>(placeholder.release()) + real_size);

			// Map right half.
			void* right = MapViewOfFile3(id->area.get(), nullptr, placeholder.get(), 0, real_size, MEM_REPLACE_PLACEHOLDER, PAGE_READWRITE, nullptr, 0);
			if (!right) {
				CLOG_THIS("Attempt %llu/%llu: MapViewOfFile3 for right half failed with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}
			id->right = std::shared_ptr<void>(right, [](void* ptr) { UnmapViewOfFile(ptr); });

			// Ownership of right half is now transferred, so clear the pointer to prevent undefined behavior.
			placeholder.release();

			done = true;
		}

		if (!done) {
			throw std::runtime_error("Failed to allocate ring buffer memory.");
		}

		// Assign the left half to the buffer pointer.
		_buffer = reinterpret_cast<T*>(id->left.get());
	} else if (IsWindowsXPOrGreater()) {
		// On Windows XP and beyond, we're extremely limited when it comes to this. Our best shot is to just attempt over and over again until it works, as we can't reserve memory and split it.

		// This can unfortunately fail, so we should retry a lot.
		bool done = false;
		for (uint64_t attempt = 1; !done && (attempt <= max_attempts); attempt++) {
			// Create a file mapping backed by the paging file. This needs to be twice as wide.
			void* filemap = CreateFileMappingW(INVALID_HANDLE_VALUE, nullptr, PAGE_EXECUTE_READWRITE, ((wide_size >> 32) & 0xFFFFFFFFull), (wide_size & 0xFFFFFFFFull), nullptr);
			if (!filemap) {
				CLOG_THIS("Attempt %llu/%llu: CreateFileMappingW failed with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}
			id->area = std::shared_ptr<void>{filemap, [](void* ptr) { CloseHandle(ptr); }};

			// Attempt to map the entire area to be allocated in one go.
			void* fullview = MapViewOfFile(filemap, FILE_MAP_ALL_ACCESS, 0, 0, wide_size);
			if (!fullview) {
				CLOG_THIS("Attempt %llu/%llu: MapViewOfFile failed with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}
			UnmapViewOfFile(fullview); // Immediately unmap, then try to map the sections individually.

			// Attempt to map the left half, if it hasn't been reallocated by another thread yet.
			void* left = MapViewOfFileEx(filemap, FILE_MAP_ALL_ACCESS, 0, 0, real_size, fullview);
			if (!left) {
				CLOG_THIS("Attempt %llu/%llu: MapViewOfFileEx for left half failed with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}
			id->left = std::shared_ptr<void>{left, [](void* ptr) { UnmapViewOfFile(ptr); }};

			// Attempt to map the right half, if it hasn't been reallocated by another thread yet.
			void* right = MapViewOfFileEx(filemap, FILE_MAP_ALL_ACCESS, 0, 0, real_size, reinterpret_cast<uint8_t*>(left) + real_size);
			if (!right) {
				CLOG_THIS("Attempt %llu/%llu: MapViewOfFileEx for right half failed with error code %lld.", attempt, max_attempts, GetLastError());
				continue;
			}
			id->right = std::shared_ptr<void>{right, [](void* ptr) { UnmapViewOfFile(ptr); }};

			// Mark as done so we leave this loop.
			done = true;
		}

		if (!done) {
			throw std::runtime_error("Failed to allocate ring buffer memory.");
		}

		_buffer = reinterpret_cast<T*>(id->left.get());
	} else { // Fall back to legacy method.
		throw std::runtime_error("Windows versions below Windows XP are just not supported.");
	}
#else
	throw std::runtime_error("Not yet implemented.");
#pragma error("Not yet implemented.");
#endif
}

template<typename T>
tonplugins::memory::ring<T>::~ring()
{
	// Literally need to do nothing!
}

template<typename T>
size_t tonplugins::memory::ring<T>::write(size_t size, T const* buffer)
{
	// Early-Exit if something is invalid.
	if (size == 0)
		return 0;

	// Limit the size of the write to the buffer size.
	size_t elements = std::min(size, _size);

	if (buffer) {
		// Copy data from the buffer into the ring.
		memcpy(_buffer + static_cast<int64_t>(_write_pos), buffer, sizeof(T) * elements);
	}

	// Advance the write position by the number of elements, wrapped into the actual buffer size.
	size_t write_old = _write_pos;
	size_t write_new = write_old + elements;
	_write_pos       = write_new % _size;

	// Advance the read position if we just overwrite part of it.
	if ((write_old < _read_pos) && (write_new >= _read_pos)) {
		// (w0 < r) && (w1 >= r), c=10
		//Read caught up to write:
		// s=5, r=0, w=0: (0 < 0) && (5 >= 0) = false (should be false)
		//Write caught up to read:
		// s=5, r=3, w=0: (0 < 3) && (5 >= 3) = true (should be true)
		_read_pos = _write_pos + 1;
		// Adding 1 allows us to differentiate between read catching up to write, and write catching up to read.
		// If write catches up to read, then read will be 1 ahead of write. Thus (w0 < r) && (w1 >= r) can be true.
		// If read catches up to write, then read will be exactly on write. Thus (w0 < r) && (w1 >= r) can be false.
		// Otherwise, we would have ambiguity between the two cases.
	}

	// Signal any listeners about the current status.
	for (auto kv : _notifications) {
		kv.second(*this);
	}

	// Return the length actually written.
	return elements;
}

template<typename T>
T const* tonplugins::memory::ring<T>::peek(size_t size)
{
	// Early-Exit if something is invalid.
	if ((size == 0) || (size > used())) {
		return nullptr;
	}

	// Calculate the pointer to return.
	T* ptr = _buffer + static_cast<int64_t>(_read_pos);

	// Return the pointer.
	return ptr;
}

template<typename T>
T* tonplugins::memory::ring<T>::poke(size_t size)
{
	// Early-Exit if something is invalid.
	if ((size == 0) || (size > this->size())) {
		return nullptr;
	}

	return _buffer + static_cast<int64_t>(_write_pos);
}

template<typename T>
size_t tonplugins::memory::ring<T>::read(size_t size, T* buffer)
{
	// Early-Exit if something is invalid.
	if (size == 0)
		return 0;

	// Limit the length of the read to the available used space.
	size = std::min(used(), size);

	if (buffer) {
		// Copy data from the ring into the buffer.
		memcpy(buffer, _buffer + static_cast<int64_t>(_read_pos), sizeof(T) * size);
	}

	// Advance the read position and wrap it back into the actual buffer size.
	_read_pos = (_read_pos + size) % _size;

	// Return the length actually read.
	return size;
}

template<typename T>
size_t tonplugins::memory::ring<T>::used()
{
	// Is the write pointer in front or on the read pointer?
	if (_write_pos >= _read_pos) {
		// If yes, just subtract the read position from the write position.
		return _write_pos - _read_pos;
	} else {
		// Otherwise, treat the write position as a number of elements, and the read position needs to be subtracted from the size.
		return _write_pos + (_size - _read_pos);
	}
}

template<typename T>
size_t tonplugins::memory::ring<T>::size()
{
	return _size;
}

template<typename T>
size_t tonplugins::memory::ring<T>::listen(ring_listener_t signal)
{
	_notifications.emplace(_notification_id++, signal);
	return (_notification_id - 1);
}

template<typename T>
void tonplugins::memory::ring<T>::silence(size_t signal)
{
	_notifications.erase(signal);
}

template class tonplugins::memory::ring<float>;
template class tonplugins::memory::ring<double>;
template class tonplugins::memory::ring<int8_t>;
template class tonplugins::memory::ring<uint8_t>;
template class tonplugins::memory::ring<int16_t>;
template class tonplugins::memory::ring<uint16_t>;
template class tonplugins::memory::ring<int32_t>;
template class tonplugins::memory::ring<uint32_t>;
template class tonplugins::memory::ring<int64_t>;
template class tonplugins::memory::ring<uint64_t>;
